---
title:   "IJCAI2015重复购买用户预测"
description:   "happy wife, happy life ..."
keywords:   ijcai
category:   Project
tags:   [ijcai] 
---


{% include JB/setup %}

###create table
首先创建一个数据库表
```
#字段含义分别为 用户id 商品id 类别id 卖家id 品牌id 时间 操作行为
create table big(
uid varchar(15),
iid varchar(15),
cid varchar(15) ,
sid varchar(15),
bid varchar(15),
time int,
action tinyint
)

#create index on field `aciton` ...
create index  index_action on big(action)

```

###load data
导入数据到数据库，这部分可以不用细看
```
mysql -u root -p tianchi2 --local-infile=1 -e 'load data local infile "/home/spark/data/ali/user.csv" into table user fields terminated by "," lines terminated by "\n"'
 
mysql -u root -p bigdata --local-infile=1 -e 'load data local infile "/home/spark/data/2015/data_format1/user_log_format1.csv" into table big fields terminated by "," lines terminated by "\n"'
```

###choose data
当前的工作主要是从数据库中进行分组查询 等到一些统计的特征进行分析，用到一些group by  ,sum,等操作。可以先了解一下是怎么个过程，具体使用可以先搁置在一边，现在主要是理解我做了写什么事情。理清整个脉络。
```
####数据格式，数据库里面的数据就是下面的这种形式 这是个示例
mysql> select * from big limit 10;
+--------+---------+------+------+------+------+--------+
| uid    | iid     | cid  | sid  | bid  | time | action |
+--------+---------+------+------+------+------+--------+
| 328862 | 323294  | 833  | 2882 | 2661 |  829 |      0 |
| 328862 | 844400  | 1271 | 2882 | 2661 |  829 |      0 |
| 328862 | 575153  | 1271 | 2882 | 2661 |  829 |      0 |
| 328862 | 996875  | 1271 | 2882 | 2661 |  829 |      0 |
| 328862 | 1086186 | 1271 | 1253 | 1049 |  829 |      0 |
| 328862 | 623866  | 1271 | 2882 | 2661 |  829 |      0 |
| 328862 | 542871  | 1467 | 2882 | 2661 |  829 |      0 |
| 328862 | 536347  | 1095 | 883  | 1647 |  829 |      0 |
| 328862 | 364513  | 1271 | 2882 | 2661 |  829 |      0 |
| 328862 | 575153  | 1271 | 2882 | 2661 |  829 |      0 |
+--------+---------+------+------+------+------+--------+
```
1、统计用户对卖家点击、收藏、购物车、购买次数
`注意，这里action行为字段的含义是0-点击，3-收藏，1-购物车、2-购买。下面也是如此。`
```
#用户对卖家点击、收藏、购物车、购买次数
select uid,sid,
sum(case when action=0 then 1 else 0 end) a , 
sum(case when action=3 then 1 else 0 end) b ,
sum(case when action=1 then 1 else 0 end) c ,
sum(case when action=2 then 1 else 0 end) d 
into outfile '/tmp/uid-sid.csv'  fields terminated by "," 
from big group by uid,sid ;
```
2、统计用户历史期间所有点击、收藏、购物车、购买次数
```
#用户历史期间所有点击、收藏、购物车、购买次数
select uid,
sum(case when action=0 then 1 else 0 end) a , 
sum(case when action=3 then 1 else 0 end) b ,
sum(case when action=1 then 1 else 0 end) c ,
sum(case when action=2 then 1 else 0 end) d 
into outfile '/tmp/uid.csv'  fields terminated by "," 
from big group by uid;
```
3、统计卖家历史期间店内商品被点击、收藏、购物车、购买次数
```
##卖家历史期间店内商品被点击、收藏、购物车、购买次数
select sid,
sum(case when action=0 then 1 else 0 end) a , 
sum(case when action=3 then 1 else 0 end) b ,
sum(case when action=1 then 1 else 0 end) c ,
sum(case when action=2 then 1 else 0 end) d 
into outfile '/tmp/sid.csv'  fields terminated by "," 
from big group by sid ;
```
4、这里本来是想统计用户在某个店内对某个品牌的操作次数与在所有店内某个品牌操作次数的比值。sql不能一次完成 所以导出数据在python里面自己处理。这里对应是添加了两个统计特征 cid-pro和bid-pro
```
#六个月内用户对品牌的操作次数
select uid,bid,count(*) 
into outfile '/tmp/uid-bid.csv'  fields terminated by "," 
from big group by uid,bid ; 

#六个月内用户对某一个具体卖家中 所有品牌的操作次数
select uid,sid,bid,count(*) 
into outfile '/tmp/uid-sid-bid.csv'  fields terminated by "," 
from big group by uid,sid,bid ; 

#六个月内用户对类别的操作次数
select uid,cid,count(*) 
into outfile '/tmp/uid-cid.csv'  fields terminated by "," 
from big group by uid,cid ; 

#六个月内用户对某一个具体卖家中 类别的操作次数
select uid,sid,cid,count(*) 
into outfile '/tmp/uid-sid-cid.csv'  fields terminated by "," 
from big group by uid,sid,cid ; 
```
这部分代码可以先不用看 你知道有cid-pro、pid-pro就可以了
```python
    uid_sid_cid=pd.read_csv("/home/spark/data/2015/data_format1/uid-sid-cid.csv",names=['user_id','merchant_id','cid','uid-sid-cid'])
    uid_cid=pd.read_csv("/home/spark/data/2015/data_format1/uid-cid.csv",names=['user_id','cid','uid-cid'])
    result=pd.merge(uid_sid_cid,uid_cid,on=['user_id','cid'])
    result['cid-pro']=result['uid-sid-cid']/result['uid-cid']
    result[['user_id','merchant_id','cid-pro']].to_csv("/home/spark/data/2015/data_format1/cid-pro.csv",index=False)
    #testreuslt.to_csv("/home/spark/data/2015/data_format1/test505.csv",index=False)

    uid_sid_bid=pd.read_csv("/home/spark/data/2015/data_format1/uid-sid-bid.csv",names=['user_id','merchant_id','bid','uid-sid-bid'])
    uid_bid=pd.read_csv("/home/spark/data/2015/data_format1/uid-bid.csv",names=['user_id','bid','uid-bid'])
    result=pd.merge(uid_sid_bid,uid_bid,on=['user_id','bid'])
    result['bid-pro']=result['uid-sid-bid']/result['uid-bid']
    result[['user_id','merchant_id','bid-pro']].to_csv("/home/spark/data/2015/data_format1/bid-pro.csv",index=False)
    #testreuslt.to_csv("/home/spark/data/2015/data_format1/test505.csv",index=False)
```
5、添加性别和年龄（从文本里面完成 没有用到数据库操作，这里不在描述过程）

---


6、这个部分现在还没有添加。。。是准备要添加的属性。。。
```
#重复购买次数
spark@spark-desktop:~/data/2015/data_format1$ head -n 100 uid-sid.csv
user_id,merchant_id,a,b,c,d
1,1019,10,0,0,4
1,1156,1,0,0,0
1,2245,5,0,0,0
1,4026,4,0,0,1
1,4177,1,0,0,0
1,4335,1,0,0,0
1,471,1,0,0,0
1,739,1,0,0,0
1,925,3,0,0,1
10,1364,13,1,0,5
10,1454,9,0,0,0


select uid,iid,count(*) 
into outfile '/tmp/uid-iid-count.csv'  fields terminated by "," 
from big where action=2 group by uid,iid ; 


d为用户在卖家店内过去购买商品的总次数   衡量重复购买次数


#重复购买时间间隔


```

###现在具有的属性（train06.csv）
`注意：前两列没有作为属性，最后一列为类别标签`
```
user_id,merchant_id,a,b,c,d,u1,u2,u3,u4,s1,s2,s3,s4,age_range,gender,cid-pro,bid-pro,label
34176,3906,36,2,0,1,410,7,0,34,14870,961,28,410,6.0,0.0,0.30499243304121354,1.0,0

34176,121,13,0,0,1,410,7,0,34,72265,2699,121,4780,6.0,0.0,1.0,1.0,0

34176,4356,12,0,0,6,410,7,0,34,6094,196,16,963,6.0,0.0,0.8181818181818182,0.7826086956521741,1
```


###links
https://dato.com/learn/userguide/

####语法使用
```python
#重命名列名字
t.rename(columns={'class':'label'},inplace=True)
#shuffle
index2=np.arange(len(t))
np.random.shuffle(index2)
t.reindex(index=index2)
```
