---
title: 决策树、CART、GBRT、Adaboost
description happy wife, happy life ...
keywords: ml,boost,gbrt
category: Study
tags: ml,boost,gbrt
---


{% include JB/setup %}


决策树是以实例为基础的归纳学习算法。 它从一组无次序、无规则的元组中推理出决策树表示形式的分类规则。它采用自顶向下的递归方式，在决策树的内部结点进行属性值的比较，并根据不同的属性值从该结点向下分支，叶结点是要学习划分的类。从根到叶结点的一条路径就对应着一条合取规则，整个决策树就对应着一组析取表达式规则。

###[ID3算法](http://dataunion.org/13680.html)
ID3算法的优点是：算法的理论清晰，方法简单，学习能力较强。
ID3算法的缺点是：只对比较小的数据集有效，且对`噪声比较敏感`，当训练数据集加大时，决策树可能会随之改变

###[C4.5](http://dataunion.org/13526.html)
C4.5是机器学习算法中的另一个分类决策树算法，它是基于ID3算法进行改进后的一种重要算法，相比于ID3算法，改进有如下几个要点：
- ID3选择属性用的是子树的信息增益，而C4.5用的是信息增益率。
- 在决策树构造过程中进行剪枝，因为某些具有很少元素的结点可能会使构造的决策树过适应（Overfitting），如果不考虑这些结点可能会更好。
- 对非离散数据也能处理。
- 能够对不完整数据进行处理。

C4.5算法的优点是：产生的分类规则易于理解，准确率较高。
C4.5算法的缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。

`信息增益比率`度量是用ID3算法中的的增益度量Gain(D，X)和分裂信息度量SplitInformation(D，X)来共同定义的。分裂信息度量SplitInformation(D，X）就相当于特征X（取值为x1，x2，……，xn，各自的概率为P1，P2，…，Pn，Pk就是样本空间中特征X取值为xk的数量除上该样本空间总数）的熵。
SplitInformation(D，X） = -P1 log2(P1)-P2 log2(P)-,…,-Pn log2(Pn)
GainRatio(D,X) = Gain(D,X)/SplitInformation(D,X)
在ID3中用信息增益选择属性时偏向于选择分枝比较多的属性值，即取值多的属性，在C4.5中由于除以SplitInformation(D,X)=H(X)，可以削弱这种作用。
另外[C4.5能够处理连续型属性](http://blog.sina.com.cn/s/blog_60acd6780100djcf.html)。而ID3只能处理离散型属性。


处理连续型属性过程
1. 对属性的取值进行排序
2. 两个属性取值之间的中点作为可能的分裂点，将数据集分成两部分，计算每个可能的分裂点的信息增益（InforGain）
3. 对每个分裂点的信息增益(InforGain)就行修正：减去log2(N-1)/|D|
4. 选择修正后信息增益(InforGain)最大的，分裂点作为该属性的`最佳分裂点`
5. 计算最佳分裂点的信息增益率（Gain Ratio）作为属性的Gain Ratio
6. 选择Gain Ratio最大的属性作为`最佳分裂属性`


###[CART分类回归树](http://dataunion.org/5771.html)
分类树：二分（简化决策树规模，提高运行效率）、GINI指数最小特征作为划分节点。  GINI值越小，表明样本的纯净度越高（即该样本只属于同一类的概率越高）。
回归树：CART则使用最小剩余方差(Squared Residuals Minimization)来决定Regression Tree的最优划分，该划分准则是`期望划分之后的子树误差方差最小`。

针对连续值处理参照c4.5做法，需要注意的地方是：
根据`离散特征分支划分数据集时，子数据集中不再包含该特征`（因为每个分支下的子数据集该特征的取值就会是一样的，信息增益或者Gini Gain将不再变化）；而根据连续特征分支时，各分支下的子数据集必须依旧包含该特征（当然，左右分支各包含的分别是取值小于、大于等于分裂值的子数据集），因为该连续特征再接下来的树分支过程中可能依旧起着决定性作用。

关于减枝--补充。。。。


###[随机森林](http://dataunion.org/12046.html)
随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。随机森林是一种统计学习理论，其随机有两个方面：首先是在训练的每一轮中，都是对原始样本集有放回的抽取固定数目的样本点，形成k个互不相同的样本集。第二点是：对于每一个决策树的建立是从总的属性中随机抽取一定量的属性作分裂属性集，这样对于k个树分类器均是不相同的。由随机生成的k个决策树组成了随机森林。对于每一个决策树来讲，其分裂属性是不断的选取具有最大信息增益的属性进行排列。整个随机森林建立后，最终的分类标准采用投票机制得到可能性最高的结果。
下图是随机森林构建的过程：

![](http://needpp.qiniudn.com/2015/05/25/71347a0c-02c2-11e5-9c7f-a12adcdca7aa.jpg)


随机森林优点：
1.通过对许多分类器进行组合，它可以产生高准确度的分类器；
2.它可以处理大量的输入变量；
3.它可以在决定类别时，评估变量的重要性；
4.在建造森林时，它可以在内部对于一般化后的误差产生不偏差的估计；
5.它包含一个好方法可以估计遗失的资料，并且，如果有很大一部分的资料遗失，仍可以维持准确度。
6.它提供一个实验方法，可以去侦测变量之间的相互作用；
7.学习过程是很快速的；
8.`对异常值和噪声具有很好的容忍度，且不容易出现过拟合`；
随机森林的缺点：
1.对于有不同级别的属性的数据，级别划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的；
2.单棵决策树的预测效果很差：由于随机选择属性，使得单棵决策树的预测效果很差。
###[AdaBoost](http://dataunion.org/5341.html)
l
###[GBRT](http://dataunion.org/5341.html)
l

