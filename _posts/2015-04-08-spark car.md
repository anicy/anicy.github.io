---
title: spark car
description happy wife, happy life ...
keywords: car,spark
category: Study
tags: car,spark
---


{% include JB/setup %}
```
bin/spark-submit --class CarCheck --master yarn-client  /home/spark/temp/lib/SparkExample.jar  hdfs://master:9000/t/newfile hdfs://master:9000/t/output2
```

```
/**
 * Created by spark on 15-3-18.
 */

import java.text.SimpleDateFormat
import scala.util.control.Breaks._
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
/** Computes an approximation to pi */
object CarCheck {
  def main(args: Array[String]) {
    if(args.length!=2){
      System.err.println("Usage : SparkDemo <input file> <output file >")
      System.exit(1)
    }
    val conf = new SparkConf().setMaster("local").setAppName("cardata")
    val spark = new SparkContext(conf)

//    val file=spark.textFile("/home/spark/data/cardata/cheliunorm.csv")
    val file=spark.textFile(args(0))

    // such as (carnum,line) like m-v
    val cars=file.map(line=>{ val fileds = line.split(",")
      (fileds(0), line)})

    val order=cars.groupByKey().map(data=>{
      //we can order by time , however the dateformat is not fit
      //repair it after coding it
      val sort=data._2.toList.sortWith(_ < _)
      (data._1,sort)
    })

    val sdf=new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")

    val keyi=order.map(data=>{
      //粤ELQ311,2013-10-8 13:16:21,卡口——兴达路贤和路口,43,2,1
      val list=data._2

      var i = 0
      var flag=true
      val distance=new Distance
      while(list.size >3 && i < list.size-1 && flag ) {
        //粤ELQ311,2013/10/8 13:16:21,卡口——兴达路贤和路口,43,2,1,112.87741,23.17866,1
        val pre = list(i).split(",")
        val cur = list(i + 1).split(",")

        val t1=pre(1)
        val lat1=pre(7).toDouble
        val lng1=pre(6).toDouble

        val t2=cur(1)
        val lat2=cur(7).toDouble
        val lng2=cur(6).toDouble


        val realtime = (sdf.parse(t2).getTime - sdf.parse(t1).getTime) / 1000

        val dis=distance.GetDistance(lat1,lng1,lat2,lng2)

//        println("dis=="+dis/33)
//        println("time=="+realtime)
        if(t1==t2){
          i=i+1
        }else {
          if(dis/100 > realtime){
            flag=false
          }
        }

        i=i+1
      }

      // check the final result
      if(flag==false){
        (data._1,"keyi","cout",flag)
      }else{
        (data._1,"normal~~~~~~~~~~~~~~",list.size,flag)
      }

    })

//
    val result=keyi.filter(x=>x._2=="keyi")

    result.collect().foreach(println)

    result.saveAsTextFile(args(1))
    print(result.count())
    spark.stop()
  }
}
```


