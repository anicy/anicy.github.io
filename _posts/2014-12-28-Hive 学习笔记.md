---
layout:  post
title:   "Hive 学习笔记"
description:   "happy wife, happy life ..."
keywords:   hive
category:   Study
tags:   [hive] 
---


{% include JB/setup %}

###数据类型
- 基本数据类型
基础数据类型包括：TINYINT,SMALLINT,INT,BIGINT,BOOLEAN,FLOAT,DOUBLE,
STRING,BINARY,TIMESTAMP,DECIMAL,CHAR,VARCHAR,DATE。

![](http://needpp.qiniudn.com/2014/12/28/04f5be48-8e62-11e4-a385-f23c9156bf7b.png)


- 复合数据类型

![](http://needpp.qiniudn.com/2014/12/28/064bac12-8e62-11e4-a385-f23c9156bf7b.png)


- 示例
```sql
CREATE TABLE employees (
    name STRING,
    salary FLOAT,
    subordinates ARRAY<STRING>,
    deductions MAP<STRING, FLOAT>,
    address STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>
) PARTITIONED BY (country STRING, state STRING);
```


###内部表和外部表
- 内部表创建
```sql
#create table 
create table dcy_test(
kakou string,
area  string,
time date,
line int,
id string,
speed int,
color string,
kind int
)
row format delimited 
fields terminated by ','
stored as textfile;
#load data from local 
load data local inpath '/home/spark/data/ETLResult.csv' into table dcy_test
```

- 外部表创建
```sql
# with the keyword `external`
create external table dcy_test3(
kakou string,
area  string,
time string,
line int,
id string,
speed int,
color string,
kind int
)
row format delimited 
fields terminated by ','
stored as textfile
location '/extends';
```

- 区别

![](http://needpp.qiniudn.com/2014/12/28/05695182-8e62-11e4-a385-f23c9156bf7b.png)



###数据导入导出
- 导入
```
#从本地文件导入
load data local inpath '/home/spark/data/ETLResult.csv' into table dcy_test3
#从hdfs导入 
load data  inpath '/home/spark/data/ETLResult.csv' into table dcy_test3
#从hive表查询方式导入(into 或者overwrite 选择一个)
insert (into/overwrite) table destion_table select id ,name from source_table
#多插入方式
from source_table 
insert into table t1
select id ,age
insert into table t2
select age,name 
#创建表导入数据
create table t3 as select id,name from source_table
```
- 导出
```
#导出到本地文件系统
insert overwrite local directory '/tmp/data/' select * from you_table
#导出到hdfs
insert overwrite  directory '/tmp/data/' select * from you_table
#指定分割符号导出
insert overwrite local directory '/tmp/local'
row format delimited
fields terminated by '\t'
select * from you_table;
#使用sql导出 在终端输入 不用进入hive shell
hive -e "select * from wyp" >> data/data3.txt
```

###数据模型
Hive中主要包含以下几种数据模型：Table（表），External Table（外部表），Partition（分区），Bucket（桶）
1、表：Hive中的表和关系型数据库中的表在概念上很类似，每个表在HDFS中都有相应的目录用来存储表的数据，这个目录可以通过${HIVE_HOME}/conf/hive-site.xml配置文件中的hive.metastore.warehouse.dir属性来配置，这个属性默认的值是/user/hive/warehouse（这个目录在HDFS上），我们可以根据实际的情况来修改这个配置。如果我有一个表wyp，那么在HDFS中会创建/user/hive/warehouse/wyp目录（这里假定hive.metastore.warehouse.dir配置为/user/hive/warehouse）；wyp表所有的数据都存放在这个目录中。这个例外是外部表。
2、外部表：Hive中的外部表和表很类似，但是其数据不是放在自己表所属的目录中，而是存放到别处，这样的好处是如果你要删除这个外部表，该外部表所指向的数据是不会被删除的，它只会删除外部表对应的元数据；而如果你要删除表，该表对应的所有数据包括元数据都会被删除。

3、分区：在Hive中，表的每一个分区对应表下的相应目录，所有分区的数据都是存储在对应的目录中。比如wyp表有dt和city两个分区，则对应dt=20131218,city=BJ对应表的目录为/user/hive/warehouse/dt=20131218/city=BJ，所有属于这个分区的数据都存放在这个目录中。
4、桶：对指定的列计算其hash，根据hash值切分数据，目的是为了并行，每一个桶对应一个文件（注意和分区的区别）。比如将wyp表id列分散至16个桶中，首先对id列的值计算hash，对应hash值为0和16的数据存储的HDFS目录为：/user/hive/warehouse/wyp/part-00000；而hash值为2的数据存储的HDFS 目录为：/user/hive/warehouse/wyp/part-00002。


![](http://needpp.qiniudn.com/2014/12/28/06ffe95c-8e62-11e4-a385-f23c9156bf7b.png)


